---
layout: post
comments: true
title: '[Paper Summary] Beyond English-Centric Multilingual Machine Translation by Facebook AI'
date: 2020-10-26 00:00:00
tags: MachineTranslation nlp
---
> Summary of Paper: Beyond English-Centric Multilingual Machine Translation


<!--more-->


{: class="table-of-content"}
* TOC
{:toc}


Paper: [Link](https://arxiv.org/abs/2010.11125) 

Code: [Link](https://github.com/pytorch/fairseq/tree/master/examples/m2m_100)

**Aim of Multilingual Translation Model (MTM):** Build a single model to translate between multiple languages

**Problems of MTM:** Performance not at par with Bilingual Models when trained on same languages. *Ex: Model trained separately on Hindi-Bangla would perform better than MTM with Hindi-Bangla as one of the pairs among many languages*. <br>**Reason:** Model capacity splits between multiple languages. 

**Solution:** Increase model capacity. <br> **Problem:** Large models required large multilingual datasets which are not easy to obtain. *Ex: Low availability of parallel data between Hindi-Bangla.*

**Solution:** Use english as pivot language. Learn models from Hindi-English and English-Bangla to convert Hindi-Bangla.<br> **Problem:** English-centric bias does not represent real-world usage of the language and hence, low performance.

Hence, this work by Facebook AI aims to learn a single MTM for non-english translation directions without using English as a pivot language.

**Novelty:** Automatic construction of parallel corpora using data-mining and semantic similarity. First Many-to-Many dataset with 7.5B training sentences for 100 languages => direct training data for thousands of translation directions (100\*99=9900 directions).

Problem with increase in no. of languages -> Quadratic increase in dataset. *Ex: 3 Languages: 3\*2 =6 directions. 4 languages: 4\*3= 16 directions.* => Model prone to underfitting. Paper proposes several scaling strategies to solve this problem.

For basics of seq-to-seq machine translation models refer to : [Visualising NMT](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

Seq-to-seq models requires a sequence of tokens as input and output. These tokens can be words, characters, sub-words etc. Providing complete words as tokens leads to large vocabularies or poor coverage. Ex: The english dictionary will have multiple forms of the root word *play* i.e. *playing, played, etc*. Hence, sub-word tokenization is preferred over word-based tokens. For detailed introduction to tokenization refer [Tokenization Summary](https://huggingface.co/transformers/tokenizer_summary.html). This paper uses [Sentencepiece tokenization](https://github.com/google/sentencepiece) as it was designed to work with languages with no segmentation.

*Problem with SentencePiece*: Generates sub-word units based on frequency in train set => Rare words & words in low-resource language less likely to be sampled.<br> Solution: Add monolingual data to train set to create the dictionary. Final dictionary contains 128k unique tokens that are well distributed across languages.

### Model details:

1. 12 Encoder - 12 Decoder layers, with 8192 FFN size and 1024 embedding dimension.
2. Parameter Count: 1.2B
3. Weight matrices of the input and output embeddings shared.
4. Adam optimizer, warmup for 4000 updates with label smoothing 0.1 .
5. For regularization, dropout parameter tuning between {0.1, 0.2, 0.3}.
6. To stabilize the training of deeper Transformers, trained with LayerDrop 0.05 and pre-normalization.

*How to effectively train with billions of sentences*?* Split the training data into 256 different shards to manage memory consumption.

*Problem with direct splitting low-resource language into shards*: Variability of each shard's data for low-resource languages. Ex: language with only 25600 sentences would contain 100 sentences of a language direction per shard => the model would easily overfit. 

Solution: each language is divided into a different number of shards based on resource level. High resource have more shards and lowest resource languages only have one shard.

### Mining Parallel Data:

Efficient way to automatically create parallel data: Search for a sentence that could be a potential translation of sentence in a monolingual corpus  (*Comparable corpora*) => Task of semantic similarity search on multiple languages. Generate embeddings by any multilingual language model and perform similarity measure. The paper uses [LASER embeddings](https://github.com/facebookresearch/faiss) and [FAISS indexing](https://github.com/facebookresearch/faiss) to create corpora.

### Postprocessing:

* Remove sentences with >50% punctuation.
* Deduplicate data, remove sentences in train set if it appears in val/test set.
* Remove long sentences: >250 subwords after segmentation.
* Language-length filtering: remove sentences with length mismatch between source-translation > 3x
* Language-specific filtering: removes sentences that contain more than 50% of characters that have not been marked as core to the identified language.

To be continued....